

## Logistic Regression
> https://zhuanlan.zhihu.com/p/32681265
solve binary classification

# Loss
def Hinge(yHat, y): # penalizes predictions y < 1
    return np.max(0, 1 - yHat * y)


## Cross Entropy
> https://zhuanlan.zhihu.com/p/51431626


# PCA
X -> Y
cov matrix: X * X^T = S âˆ‘ S ^ -1  
new cov matrix = Y * Y^T = âˆ‘, all covariance are 0 

# Empirical Parameters
* May want to get everything into -1 to +1 range (approximately)
Want to avoid large ranges, small ranges or very different ranges from one another
Rule a thumb regarding acceptable ranges
-3 to +3 is generally fine - any bigger bad
-1/3 to +1/3 is ok - any smaller bad


# Classification 
### Algorithm predicts some value for class, predicting a value for each example in the test set
* Considering this, classification can be
1. True positive (we guessed 1, it was 1)
2. False positive (we guessed 1, it was 0)
3. True negative (we guessed 0, it was 0)
4. False negative (we guessed 0, it was 1)

### Precision (TP / We Guessed True)
* How often does our algorithm cause a false alarm?
* The higher precision, the smaller false positive
* Of all patients we predicted have cancer, what fraction of them actually have cancer
    * = true positives / # predicted positive
    * = true positives / (true positive + false positive)
* High precision is good (i.e. closer to 1)
* You want a big number, because you want false positive to be as close to 0 as possible

### Recall (TP / Real True)
* How sensitive is our algorithm?
* The higher recall, the smaller false negative
* Of all patients in set that actually have cancer, what fraction did we correctly detect
    * = true positives / # actual positives
    * = true positive / (true positive + false negative)
* High recall is good (i.e. closer to 1)
* You want a big number, because you want false negative to be as close to 0 as possible

### F1Score (fscore)
* 2 * (PR/ [P + R])
* Fscore is like taking the average of precision and recall giving a higher weight to the lower value
Many formulas for computing comparable precision/accuracy values
If P = 0 or R = 0 the Fscore = 0
If P = 1 and R = 1 then Fscore = 1
The remaining values lie between 0 and 1 

### ROC Curve (Receiver Operating Characteristic)
> ROC curve the left up, the better classification
> classifiers should be in left up area, (divided by (0,0) (1,1) diagnol line, random classifier) 

TPR = TP / (TP + FN)
FPR = FP / (TN + FP)

   TPR
    |
1.0 |(all samples are                    (all samples are
    | correctly classified)               classified as positive)
    |
    |
    |
    |
    |(all samples are                    (all samples are
    | classified as negative)             wrongly classified)
 0  ------------------------------------------------------------- FPR
     0                                                        1.0

* AUC  (Area Under roc Curve) 
    - usually between 0.5 - 1.0
    - the larger, the better 

> computation: https://www.zhihu.com/search?type=content&q=auc%20roc%20curve


### Calibration
* when to use: Thereâ€™s no point in calibrating if the classifier is already good in this respect. First make a reliability diagram and if it looks like it could be improved, then calibrate. That is, if your metric justifies it.
* reliability diagram: x mean predicted value for each bin, y: fraction of true positive cases.
* algorithms: http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/
    - platts: logistic regression on the output of the SVM with respect to the true class labels

        from sklearn.linear_model import LogisticRegression as LR

        lr = LR()                                                       
        lr.fit( p_train.reshape( -1, 1 ), y_train )     # LR needs X to be 2-dimensional
        p_calibrated = lr.predict_proba( p_test.reshape( -1, 1 ))[:,1]

    - isotonic

        from sklearn.isotonic import IsotonicRegression as IR

        ir = IR( out_of_bounds = 'clip' )
        ir.fit( p_train, y_train )
        p_calibrated = ir.transform( p_test )   # or ir.fit( p_test ), that's the same thing

* motivation of calibration: https://www.quora.com/What-is-called-classifier-calibration-in-machine-learning
    - e.g. if your classifier say there're 90% possibility that these events should be blocked, yep it should be. So you collect all prediction with 0.9 and see their true label, ideally you might want to see 90% fraud cases within them.
* https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/
* The distribution of the probabilities can be adjusted to better match the expected distribution observed in the data. This adjustment is referred to as calibration, as in the calibration of the model or the calibration of the distribution of class probabilities.
* Empirical results suggest that unlike logistic regression which directly predicts probabilities, SVMs, bagged decision trees, and random forests can benefit the more from calibrating predicted probabilities. They not natively predict probabilities, meaning the probabilities are often uncalibrated.

##### Logic
###### sufficient and necessary
* P -> Q, P is sufficient to Q 
P | ... | ... -> Q
e.g. live 

* ~Q -> ~P (thus, P -> Q), P is necessary to Q
P & ... & ... -> Q

* P <-> Q

(~P â†’ ~Q) & (P â†’ Q)
(Q â†’ P) & (P â†’ Q)
P â†” Q

## Linear Regression
## R square
* http://statisticsbyjim.com/regression/interpret-r-squared-regression/
* Usually, the larger the R2, the better the regression model fits your observations. 
* R^2 = variance explained by the model / total variance
* 0% represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.
* 100% represents a model that explains all of the variation in the response variable around its mean.


## Preprocessing
* randomize order


# Validation
* Given a training set instead split into three pieces
    1. Training set (60%) - m values
    2. Cross validation (CV) set (20%)mcv
    3. Test set (20%) mtest 
* Model Selection by Best E_val, select with Eval(Am(Dtrain)) while returning Amâˆ— (D)
* Estimate generalization error of model using the test set
* In machine learning as practiced today - many people will select the model using the test set and then check the model is OK for generalization using the test error (which we've said is bad because it gives a bias analysis)


## Bias v.s. Variance
> Baggingæ˜¯bootstrap+aggregationï¼Œå¯¹æ ·æœ¬çš„ä¸æ–­é‡é‡‡æ ·å’Œèšåˆè®­ç»ƒï¼Œä¸æ–­å‡å°‘äº†æ¯ä¸ªæ ·æœ¬çš„å½±å“ï¼Œå¢åŠ äº†æ€»ä½“å¯¹äºå„ä¸ªæ ·æœ¬çš„æŠ—å¹²æ‰°èƒ½åŠ›ï¼Œæ‰€ä»¥æ˜¯é™ä½variance> ã€‚
> è€ŒBoostingæ˜¯ä¸²è¡Œçš„è¿­ä»£ç®—æ³•ï¼Œä¸æ–­å»æ”¹è¿›æ‹Ÿåˆæ¯ä¸€ä¸ªæ ·æœ¬ï¼Œè‡ªç„¶å¯¹æ ·æœ¬çš„ä¾èµ–æ€§å¼ºï¼Œè™½ç„¶å¯ä»¥å¾ˆå¥½çš„æ‹Ÿåˆæ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œä½†æ˜¯å¦‚æœè®­ç»ƒæ•°æ®æœ‰é—®é¢˜ï¼Œåˆ™æ¨¡å‹ä¼šå¾ˆå·®ï¼Œbiasä¹Ÿå°±å¾ˆé«˜ã€‚


## Debugging a Machine Learning Algorithm
* Plot h_train(x), h_cv(x) learning curve

### Fix high bias
* If an algorithm is already suffering from high bias, more data does not help
* Get additional features 
* Kernel feature transform
* Decrease regularization

### Fix high variance
* Get more data
* Try smaller features

### Unbalanced Data
* Over-sampling
* Down-sampling


## Boosting
* Ensemble weak classifiers 

### AdaBoost
* update data distribution
    - decrease good classified data probability
    - increase poor classified data probability

### Boostrap Aggregation (Bagging)
> Re-sample N examples from D uniformly with replacement
* bagging works reasonably well if base algorithm sensitive to data randomness


## Models
### Decision Tree
* implementation
* https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/
     
### Random Forest
* Bagging(reduce variance) + random-subspace C&RT decision tree(large variance)


### ResNet

### Inception


## Applications
### Recommender System

#### Content-Based
* offline computation, items features don't chnage that frequently
* less surprise
> assume we have features regarding the content which will help us identify things that make them appealing to a user
D = item data
v: item features
u: user preference vector
rate = u * v
Want to train u, we treat each rate as label, 
then training u with rates, and predict unrated by trained u * v

e.g. recommend articles by making document features as word tf-idf representation

#### User-Based
* usually online compute
* complicated
* can bring user surprises

#### Collaborative Filtering
> we don't have all features to do content-based recommendation
> https://zhuanlan.zhihu.com/p/25069367


### Anomaly Detection
> we have a dataset containg normal data
1. train the probability p(x), test p(x) < threshold == anomaly 
2. Multivariate Guassian p(x; u, âˆ‘)
    * standard deviation = 1/m âˆ‘ (x_i - u)^2 -> (x_i - u) ^ T * (x_i - u)

### Active Learning
https://blog.csdn.net/Houchaoqun_XMU/article/details/80146710
> Use complicated teacher output to train thinner student network
å¦‚ä¸‹å›¾æ‰€ç¤ºä¸ºå¸¸è§çš„ä¸»åŠ¨å­¦ä¹ æµç¨‹å›¾ï¼Œå±äºä¸€ä¸ªå®Œæ•´çš„è¿­ä»£è¿‡ç¨‹ï¼Œæ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º A = (C, L, S, Q, U)ã€‚å…¶ä¸­Cè¡¨ç¤ºåˆ†ç±»å™¨ï¼ˆ1ä¸ªæˆ–è€…å¤šä¸ªï¼‰ã€Lè¡¨ç¤ºå¸¦æ ‡æ³¨çš„æ ·æœ¬é›†ã€Sè¡¨ç¤ºèƒ½å¤Ÿæ ‡æ³¨æ ·æœ¬çš„ä¸“å®¶ã€Qè¡¨ç¤ºå½“å‰æ‰€ä½¿ç”¨çš„æŸ¥è¯¢ç­–ç•¥ã€Uè¡¨ç¤ºæœªæ ‡æ³¨çš„æ ·æœ¬é›†ã€‚æµç¨‹å›¾å¯è§£é‡Šä¸ºå¦‚ä¸‹æ­¥éª¤ï¼ˆä»¥åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼‰ï¼š

ï¼ˆ1ï¼‰é€‰å–åˆé€‚çš„åˆ†ç±»å™¨ï¼ˆç½‘ç»œæ¨¡å‹ï¼‰è®°ä¸º current_modelÂ ã€ä¸»åŠ¨é€‰æ‹©ç­–ç•¥ã€æ•°æ®åˆ’åˆ†ä¸º train_sampleï¼ˆå¸¦æ ‡æ³¨çš„æ ·æœ¬ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ï¼‰ã€validation_sampleï¼ˆå¸¦æ ‡æ³¨çš„æ ·æœ¬ï¼Œç”¨äºéªŒè¯å½“å‰æ¨¡å‹çš„æ€§èƒ½ï¼‰ã€active_sampleï¼ˆæœªæ ‡æ³¨çš„æ•°æ®é›†ï¼Œå¯¹åº”äºublabeled poolï¼‰ï¼›

ï¼ˆ2ï¼‰åˆå§‹åŒ–ï¼šéšæœºåˆå§‹åŒ–æˆ–è€…é€šè¿‡è¿ç§»å­¦ä¹ ï¼ˆsource domainï¼‰åˆå§‹åŒ–ï¼›å¦‚æœæœ‰target domainçš„æ ‡æ³¨æ ·æœ¬ï¼Œå°±é€šè¿‡è¿™äº›æ ‡æ³¨æ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›

ï¼ˆ3ï¼‰ä½¿ç”¨å½“å‰æ¨¡å‹ current_model å¯¹Â active_sample ä¸­çš„æ ·æœ¬è¿›è¡Œé€ä¸€é¢„æµ‹ï¼ˆé¢„æµ‹ä¸éœ€è¦æ ‡ç­¾ï¼‰ï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœã€‚æ­¤æ—¶å¯ä»¥é€‰æ‹© Uncertainty Strategy è¡¡é‡æ ·æœ¬çš„æ ‡æ³¨ä»·å€¼ï¼Œé¢„æµ‹ç»“æœè¶Šæ¥è¿‘0.5çš„æ ·æœ¬è¡¨ç¤ºå½“å‰æ¨¡å‹å¯¹äºè¯¥æ ·æœ¬å…·æœ‰è¾ƒé«˜çš„ä¸ç¡®å®šæ€§ï¼Œå³æ ·æœ¬éœ€è¦è¿›è¡Œæ ‡æ³¨çš„ä»·å€¼è¶Šé«˜ã€‚

ï¼ˆ4ï¼‰ä¸“å®¶å¯¹é€‰æ‹©çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶å°†æ ‡æ³¨åçš„æ ·æœ¬æ”¾è‡³train_sapmleç›®å½•ä¸‹ã€‚

ï¼ˆ5ï¼‰ä½¿ç”¨å½“å‰æ‰€æœ‰æ ‡æ³¨æ ·æœ¬ train_sampleå¯¹å½“å‰æ¨¡å‹current_model è¿›è¡Œfine-tuningï¼Œæ›´æ–° current_modelï¼›

ï¼ˆ6ï¼‰ä½¿ç”¨ current_model å¯¹validation_sampleè¿›è¡ŒéªŒè¯ï¼Œå¦‚æœå½“å‰æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°ç›®æ ‡æˆ–è€…å·²ä¸èƒ½å†ç»§ç»­æ ‡æ³¨æ–°çš„æ ·æœ¬ï¼ˆæ²¡æœ‰ä¸“å®¶æˆ–è€…æ²¡æœ‰é’±ï¼‰ï¼Œåˆ™ç»“æŸè¿­ä»£è¿‡ç¨‹ã€‚å¦åˆ™ï¼Œå¾ªç¯æ‰§è¡Œæ­¥éª¤ï¼ˆ3ï¼‰-ï¼ˆ6ï¼‰ã€‚

### Distillation
> https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322
> model compression
Ensemble æ˜¯ä¸€ç§å¾ˆå¸¸ç”¨çš„æå‡æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ï¼Œ ç®€å•æ¥è¯´å°±æ˜¯å¯¹åŒæ ·çš„æ•°æ®è®­ç»ƒå¤šä¸ªä¸åŒç§çš„æ¨¡å‹ï¼Œ ç„¶åå¯¹å®ƒä»¬çš„é¢„æµ‹å€¼å–å¹³å‡ã€‚ä½†æ˜¯Ensembleçš„æ–¹æ³•ä»£ä»·å¤ªå¤§ã€‚å‰äººçš„å·¥ä½œæœ‰éªŒè¯å¯ä»¥å°†ensembleæ¨¡å‹ä¸­çš„knowledgeå‹ç¼©åˆ°ä¸€ä¸ªå•ä¸€ç®€å•æ¨¡å‹ä¸­

* Once the cumbersome model has been trained, we can then use a different kind of training, which we call â€œdistillationâ€ to transfer the knowledge from the cumbersome model to a small model that is more suitable for deployment

* soft targets generated from cumbersome model provides higher entropy comparing with hard targets, making small model be trained on these smaller dataset + higher learning rate

* In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1.

* Approach
    Caruana and his collaborators circumvent this problem by using the logits (the inputs to the final
    softmax) rather than the probabilities produced by the softmax as the targets for learning the small
    model and they minimize the squared difference between the logits produced by the cumbersome
    model and the logits produced by the small model. Our more general solution, called â€œdistillationâ€,
    is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft
    set of targets. We then use the same high temperature when training the small model to match these
    soft targets. We show later that matching the logits of the cumbersome model is actually a special
    case of distillation.

* intermediate temperatures work best which strongly suggests that ignoring the large negative logits can be helpful     

* Maybe private dataset can use this similar techniques, by choosing right bias:
    We then tried omitting all examples of the digit 3 from the transfer set. So from the perspective
    of the distilled model, 3 is a mythical digit that it has never seen. Despite this, the distilled model
    only makes 206 test errors of which 133 are on the 1010 threes in the test set. Most of the errors
    are caused by the fact that the learned bias for the 3 class is much too low. If this bias is increased
    by 3.5 (which optimizes overall performance on the test set), the distilled model makes 109 errors
    of which 14 are on 3s. So with the right bias, the distilled model gets 98.6% of the test 3s correct
    despite never having seen a 3 during training. If the transfer set contains only the 7s and 8s from the
    training set, the distilled model makes 47.3% test errors, but when the biases for 7 and 8 are reduced
    by 7.6 to optimize test performance, this falls to 13.2% test errors.

    - After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the
specialist class is oversampled

## Bayes Theorem
* è§£æ±º"é€†æ¦‚ç‡"å•é¡Œ, åœ¨æœ‰é™çš„ä¿¡æ¯ä¸‹ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬é¢„æµ‹å‡ºæ¦‚ç‡ã€‚ https://zhuanlan.zhihu.com/p/37768413
* æˆ‘ä»¬å…ˆæ ¹æ®ä»¥å¾€çš„ç»éªŒé¢„ä¼°ä¸€ä¸ª"å…ˆéªŒæ¦‚ç‡"P(A)ï¼Œç„¶ååŠ å…¥æ–°çš„ä¿¡æ¯ï¼ˆå®éªŒç»“æœBï¼‰ï¼Œè¿™æ ·æœ‰äº†æ–°çš„ä¿¡æ¯åï¼Œæˆ‘ä»¬å¯¹äº‹ä»¶Açš„é¢„æµ‹å°±æ›´åŠ å‡†ç¡®ã€‚
    - åéªŒæ¦‚ç‡ï¼ˆæ–°ä¿¡æ¯å‡ºç°åAå‘ç”Ÿçš„æ¦‚ç‡ï¼‰ã€€ï¼ã€€å…ˆéªŒæ¦‚ç‡ï¼ˆAå‘ç”Ÿçš„æ¦‚ç‡ï¼‰ ï½˜ å¯èƒ½æ€§å‡½æ•°ï¼ˆæ–°ä¿¡æ¯å¸¦å‡ºç°æ¥çš„è°ƒæ•´ï¼‰

* è´å¶æ–¯çš„åº•å±‚æ€æƒ³å°±æ˜¯ï¼š
    - å¦‚æœæˆ‘èƒ½æŒæ¡ä¸€ä¸ªäº‹æƒ…çš„å…¨éƒ¨ä¿¡æ¯ï¼Œæˆ‘å½“ç„¶èƒ½è®¡ç®—å‡ºä¸€ä¸ªå®¢è§‚æ¦‚ç‡ï¼ˆå¤å…¸æ¦‚ç‡ã€æ­£å‘æ¦‚ç‡ï¼‰ã€‚å¯æ˜¯ç”Ÿæ´»ä¸­ç»å¤§å¤šæ•°å†³ç­–é¢ä¸´çš„ä¿¡æ¯éƒ½æ˜¯ä¸å…¨çš„ï¼Œæˆ‘ä»¬æ‰‹ä¸­åªæœ‰æœ‰é™çš„ä¿¡æ¯ã€‚æ—¢ç„¶æ— æ³•å¾—åˆ°å…¨é¢çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å°±åœ¨ä¿¡æ¯æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå°½å¯èƒ½åšå‡ºä¸€ä¸ªå¥½çš„é¢„æµ‹ã€‚ä¹Ÿå°±æ˜¯ï¼Œåœ¨ä¸»è§‚åˆ¤æ–­çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥å…ˆä¼°è®¡ä¸€ä¸ªå€¼ï¼ˆå…ˆéªŒæ¦‚ç‡ï¼‰ï¼Œç„¶åæ ¹æ®è§‚å¯Ÿçš„æ–°ä¿¡æ¯ä¸æ–­ä¿®æ­£(å¯èƒ½æ€§å‡½æ•°)ã€‚
    - çµ¦ä¸€å€‹ä¸»è§€empiricalåˆ¤æ–·(prior)ï¼Œæ ¹æ“šå®¢è§€äº‹å¯¦(æ•¸æ“š, liklihood)ä¿®æ­£å¾—åˆ°posterior

    P(Y|X) = P(Y) * P(X|Y) / P(X)

P(Y): prior, without knowing anything
P(X|Y)/P(X): likelihood, make prior approximates posterior. ä¼¼ç„¶ï¼Œå³ P(X|Y) ï¼Œæ˜¯å‡è®¾ Y å·²çŸ¥åæˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ•°æ®åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­çš„
    * likelihood > 1: P(Y) is enhanced
    * likelihood = 1: X is useless for determining Y
    * likelihood < 1: X decreases P(Y)
P(Y|X): posterior 

P(X): often be solved by total probability
    - å…¨æ¦‚ç‡å°±æ˜¯è¡¨ç¤ºè¾¾åˆ°æŸä¸ªç›®çš„ï¼Œæœ‰å¤šç§æ–¹å¼ï¼ˆæˆ–è€…é€ æˆæŸç§ç»“æœï¼Œæœ‰å¤šç§åŸå› ï¼‰ï¼Œé—®è¾¾åˆ°ç›®çš„çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼ˆé€ æˆè¿™ç§ç»“æœçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼‰
    P(S) =    P(Sã„‡A)    +      P(Sã„‡B)  +     P(Sã„‡C)   + ...
         = P(S|A) * P(A) + P(S|A) * P(A) + P(S|A) * P(A) + ...


* MLE v.s. MAP https://zhuanlan.zhihu.com/p/32480810
é¢‘ç‡å­¦æ´¾ - Frequentist - Maximum Likelihood Estimation (MLEï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡)
è´å¶æ–¯å­¦æ´¾ - Bayesian - Maximum A Posteriori (MAPï¼Œæœ€å¤§åéªŒä¼°è®¡)

    - éšç€æ•°æ®é‡çš„å¢åŠ ï¼Œå‚æ•°åˆ†å¸ƒä¼šè¶Šæ¥è¶Šå‘æ•°æ®é æ‹¢ï¼Œå…ˆéªŒçš„å½±å“åŠ›ä¼šè¶Šæ¥è¶Šå°
    - å¦‚æœå…ˆéªŒæ˜¯uniform distributionï¼Œåˆ™è´å¶æ–¯æ–¹æ³•ç­‰ä»·äºé¢‘ç‡æ–¹æ³•ã€‚å› ä¸ºç›´è§‚ä¸Šæ¥è®²ï¼Œå…ˆéªŒæ˜¯uniform distributionæœ¬è´¨ä¸Šè¡¨ç¤ºå¯¹äº‹ç‰©æ²¡æœ‰ä»»ä½•é¢„åˆ¤
    - MAP=MLE + Regularization

#### Naive Bayes
> computation https://zhuanlan.zhihu.com/p/37575364

P(Y|X) = P(Y) * P(X|Y) / P(X)

arg max P(Y|X) = arg max P(Y) * P(X | Y) since P(X) isn't related to Y
               = arg max P(Y, X1, X2, ... Xn)
               = arg max P(Y) * P(X1|Y) * P(X2|Y) * ... * P(Xn|Y) by independent assumptoin

NaiveBayesClassifier(Y = Yi) = arg max P(Yi) * ã„‡P(Xj|Yi)

### Baysian Network
TO READ:
> https://blog.csdn.net/zdy0_2004/article/details/41096141
> https://zhuanlan.zhihu.com/p/30139208

## Semi-supervised Learning

### Pre-training as regularization
> http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf
To simply state that unsupervised pretraining is a regularization strategy somewhat undermines the significance of its effectiveness. Not
all regularizers are created equal and, in comparison to standard regularization schemes such as
L1 and L2 parameter penalization, unsupervised pre-training is dramatically effective.

### Pseudo-Labeling 
> Entropy Minimization
1. Take the same model that you used with your training set and that gave you good results.
2. Use it now with your unlabeled test set to predict the outputs ( or pseudo-labels).  We donâ€™t know if these predictions are correct, but we do now have quite accurate labels and thatâ€™s what we aim in this step.
3. Concatenate the training labels with the test set pseudo labels.
4. Concatenate the features of the training set with the features of the test set.
5. Finally, train the model in the same way you did before with the training set.


* Use denoising auto-encoder + dropout to boost performance

### Ladder Networks

### Co-Training
> multiple models to pseudo-label unlabeled data, augmenting dataset with pseudo-labels
Co-trainingæœ‰ m1 å’Œ m2 ä¸¤ä¸ªæ¨¡å‹ï¼Œå®ƒä»¬åˆ†åˆ«åœ¨ä¸åŒçš„ç‰¹å¾é›†ä¸Šè®­ç»ƒã€‚æ¯è½®è¿­ä»£ä¸­ï¼Œå¦‚æœä¸¤ä¸ªæ¨¡å‹é‡Œçš„ä¸€ä¸ªï¼Œæ¯”å¦‚æ¨¡å‹ m1 è®¤ä¸ºè‡ªå·±å¯¹æ ·æœ¬ x çš„åˆ†ç±»æ˜¯å¯ä¿¡çš„ï¼Œç½®ä¿¡åº¦é«˜ï¼Œåˆ†ç±»æ¦‚ç‡å¤§äºé˜ˆå€¼ r ï¼Œé‚£  m1 ä¼šä¸ºå®ƒç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç„¶åæŠŠå®ƒæ”¾å…¥ m2 çš„è®­ç»ƒé›†ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä¸€ä¸ªæ¨¡å‹ä¼šä¸ºå¦ä¸€ä¸ªæ¨¡å‹çš„è¾“å…¥æä¾›æ ‡ç­¾ã€‚

#### Multi-View Learning
> å¯¹ç‰¹å¾è¿›è¡Œæ‹†åˆ†ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼Œæ¥ä¿è¯æ¨¡å‹é—´çš„å·®å¼‚æ€§ã€‚

#### Single-View
> é‡‡ç”¨ä¸åŒç§ç±»çš„æ¨¡å‹ï¼Œä½†æ˜¯é‡‡ç”¨å…¨éƒ¨ç‰¹å¾ï¼Œä¹Ÿæ˜¯å¯ä»¥çš„ã€‚åŸºäºåä¸€ç§æ–¹æ³•ï¼Œå¥½å¤šå¼€å§‹åšé›†æˆæ–¹æ³•ï¼Œé‡‡ç”¨boostingæ–¹å¼ï¼ŒåŠ å…¥æ›´å¤šåˆ†ç±»å™¨ï¼Œå½“ç„¶ä¹Ÿæ˜¯å¯ä»¥åŒæ—¶åšç‰¹å¾çš„é‡‡æ ·ã€‚



## Deep Learning Training Details

### Back-propogation
* function derivatives

* gradient descent
> https://zhuanlan.zhihu.com/p/32230623


### Normalization
> https://zhuanlan.zhihu.com/p/33173246

* intention:
ï¼ˆ1ï¼‰å»é™¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ â€”> ç‹¬ç«‹ï¼›
ï¼ˆ2ï¼‰ä½¿å¾—æ‰€æœ‰ç‰¹å¾å…·æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·® â€”> åŒåˆ†å¸ƒã€‚
 (3) solve gradient vanishing problems

    - however, if normalizing to N(0, 1), the input of activation function would be in the non-saturating (linear) region, lossing expression capability. Thus, BatchNorm standardization then re-shift.

* Batch Norm to reduce influence on weight init

    import tensorflow as tf
    # put this before nonlinear transformation
    layer = tf.contrib.layers.batch_norm(layer, center=True, scale=True,
                                     is_training=True)

    - back propogationæ™‚é‚„èƒ½è§£è€¦åˆ


### Weight Initialization 
https://zhuanlan.zhihu.com/p/25110150

### Activation Function

* saturating - A saturating activation function squeezes the input
> https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean
    ğ‘“ is non-saturating iff (|limğ‘§â†’âˆ’âˆğ‘“(ğ‘§)|=+âˆ)âˆ¨|limğ‘§â†’+âˆğ‘“(ğ‘§)|=+âˆ)  


#### ReLU

* use variation of Xavier Initialization

    import numpy as np
    W = np.random.randn(node_in, node_out) / np.sqrt(node_in / 2)

### Regularization

* Dropout 
    - can be viewed as a way of training an exponentially large ensemble of models that share weights.

